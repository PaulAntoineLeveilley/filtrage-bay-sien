\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin=2.5cm}
\usepackage{mathtools} %\usepackage{align}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{bbold}

\title{SOD333 - Rapport}
\author{Paul-Antoine Leveilley \& Mila Rocco}
\date{Septembre 2022}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage







\newpage
\section{TP1: calcul d'une intégrale par méthode de Monte-Carlo, échantillonage préférentiel.}
\subsection{Introduction}
Dans ce TP, on se propose de calculer une intégrale par méthode de Monte Carlo. De manière générale, la méthode de Monte Carlo
consisite à approcher l'intégrale : 
\[\mu = \int g(x)q(x)dx\] 
Ou $q$ est une densité, par l'estimateur : 
\begin{equation}
  \label{estim}
  \hat{\mu}_N = \frac{1}{N} \sum_{i=1}^N g(X_i)
\end{equation}
En effet, d'après la loi forte des grands nombres, 
\[\hat{\mu}_N\underset{p.s.}{\longrightarrow}\mu \] 

Pour illustrer cette méthode, nous allons estimer l'intégrale 
\[\int_0^1 cos(\frac{\pi x}{2})dx\]
Dont la valeur se calcule analytiquement, et vaut $\frac{2}{\pi}$ \\
Dans un premier temps, on mettra en oeuvre la méthode dite "naive" de Monte Carlo. Des considérations de minimisation de
la variance de l'estimateur nous conduirons à employer une méthode méthode 
meilleure. Ce sera l'objet de la deuxième partie.\\ 

 \subsection{Calcul par méthode de Monte Carlo naive}
Une première diée est de choisir la loi uniforme sur $[0,1]$, qui admet pour densité $q(x) = \mathbb{1}_{x \in [0,1]}$, ainsi que $g(x) = \cos (\frac{\pi x}{2})$
\subsubsection{Calcul préliminaire : variance de l'estimateur naif.}
Comme expliqué précédemment, la variance de l'estimation 
est un indicateur clé pour nous. On cherche a ce qu'elle soit la plus petite possible, 
pour avoir une précision donnée sur le calcul de l'intégrale, en un nombre
minimum d'itérations. Employons nous donc à calculer cette variance pour (\ref{estim}). 
\begin{align*} 
   Var(\hat{\mu}_N) &= \frac1{N^2} \sum_{i=1}^N Var(g(X_i))\\ 
   &= \frac1{N^2}\times N Var(cos(\frac{\pi X}2)) \\ 
   &= \frac1{N} \left ( \int_0^1 cos^2(\frac{\pi x}2)dx - \left ( \int_0^1 cos(\frac{\pi x}2)dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{1+cos(\pi x)}{2}dx - 
    \frac4{\pi^2} \right )\\
   &= \frac1{N} \left ( \frac{1}{2} - 
    \frac4{\pi^2} \right )\\
 \end{align*}
\subsubsection{Simulations}
Dans l'intégralité de ce TP, on prendra $N=50$.
Pour réaliser cette estimation de $\mu$, nous simulons en machine
le tirage d'un échantillon $(X_{0},\textrm{...},X_{N})$ iid de loi uniforme sur $[0,1]$.
Ensuite, nous calculons une estimation de $\mu$ en utilisant l'estimateur (\ref{estim})
Nous réalisons ce calcul 10 fois, avec des échantillons différents. Le tableau (\ref{naif}) 
présente les résultat.
\begin{figure}[h!]
  \centering
  \caption{Calcul de l'estimation naive de $\mu$ pour 10 échantillons différents}
  \label{naif}
  \begin{tabular}{|*{11}{c|}}
    \hline 0.6685 & 0.67890& 0.69391& 0.59470& 0.69625&0.6424 & 0.64359& 0.61404& 0.65725& 0.63069 \\
    \hline
  \end{tabular}
\end{figure}

Nous observons que le résultat est assez proche de la valeur exacte de l'intégrale 
($\frac{2}{\pi} \approx 0.6366$). Cela dit, on constate également une forte variance des résultats de l'estimation. Nous avons cherché à 
estimer cette variance pour un nombre de $1000$ réalisations de l'estimation $\hat{\mu}_{N}$. Nous obtenons une variance empirique de $Var_{emp}(\mu_{N})=0.00184$
Ceci est conforme à ce qui est attendu. Le calcul de la partie précédente prédit en effet :  $Var(\hat{\mu}_{N})\approx \frac{9,47.10^{-2}}{N} \approx 1,89.10^{-3})$


\subsubsection{Théorème central limite : illustration dans notre cas}

Le théorème centrale limite prédit que l'erreur normalisée doit se comporter commme une loi 
normal centrée réduite pour $N \rightarrow \infty$. C'est à dire : 
\[\frac {\hat{\mu}_{N}-\mu }{\sigma /{\sqrt {N}}}\underset{\mathcal{L} }{\longrightarrow}\mathcal{N} (0,1)\]

Pour illustrer ceci, nous estimons la distribution de l'erreur renormalisée
par l'histogramme de 1000 réalisations de l'erreur renormalisée empirique. Nous présentons
ce résultat sur le graphique de la figure \ref{TP1_MC_TCL}, en superposant l'histogramme avec la représentation graphique de la densité de la gaussienne centrée réduite.
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP1/MC_brute_TCL.png}
\caption{Distribution empirique de l'erreur renormalisée et densité de la gaussienne centrée réduite}
\label{TP1_MC_TCL}
\end{figure}

Nous constatons que, comme le prédit le TCL, l'erreur renormalisée possède une distribution proche d'une gaussienne centrée réduite.



\subsection{Echantillonage pondéré et réduction de la variance}
L'estimation de la partie précédente conduit à une grande variance du résultat.
Ce n'est pas satisfaisant et l'on peut faire mieux. En effet, quand nous estimons 
$\mu$ avec (\ref{estim}), nous donnons autant d'importance a chaque réalisation
de $X_{i}$ dans l'intégrale. Mais certains termes ont une contribution plus
imoprtante dans le calcul. Ce sont les termes pour lesquels
$g(X_{i})$ est grand. Intuitivement, nous devons donc chercher à estimer "finement" les grandes valeurs de $gq$. Il faut donc 
tirer selon une distribution "proche" de $gq$. C'est l'idée de l'échantillonage pondéré (préférentiel).\\

L'échantillonage pondéré consisite à remplacer $\hat{\mu}_{N}$ par : 
\[\tilde{\mu}_{N}= \frac1{N} \sum_{i=1}^N g(X_i)\frac{q(X_i)}{\Tilde{q}(X_i)}\]
Ou, $\tilde{q}$ est une densite et $(X_{i})_{1\leq i \leq n }$ est distribué selon une loi de densité $\tilde{q}$ 


En effet, dans ce cas, par loi forte des grands nombres, $\tilde{\mu}_{N}$ converge presque surement vers : 
\[\mathbb{E}^{\mathbb{Q}}[g(X)\frac{q(X)}{\tilde{q}(X)}]=\int_0^1 \frac{g(x)q(x)}{\Tilde{q}(x)} \Tilde{q}(x) dx = \int_0^1 g(x)q(x)dx = \mu \]
 Ou $\mathbb{E}^{\mathbb{Q}}$ désigne l'espérence sous la nouvelle probabilité $\mathbb{Q}$. Pour cette raison, on 
 appelle également cette technique "changement" de probabilité.

Maintenant, puisque c'est cela que nous cherchons à optimiser, calculons la variance de ce nouvel estimateur.
\begin{align*} 
   Var(\tilde{\mu}_N) &= \frac1{N^2} \sum_{i=1}^N Var \left( g(X_i)\frac{q(X_i)}{\Tilde{q}(X_i)} \right)\\ 
   &= \frac1{N} \left ( \int_0^1 \left (\frac{g(x)q(x)}{\Tilde{q}(x)} \right )^2 \Tilde{q}(x) dx  - \left ( \int_0^1 \frac{g(x)q(x)}{\Tilde{q}(x)} \Tilde{q}(x) dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{g^2(x)q^2(x)}{\Tilde{q}(x)} dx  - \left ( \int_0^1 g(x)q(x) dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{g^2(x)q^2(x)}{\Tilde{q}(x)} dx  - \mu^{2} \right )\\
   % &= \frac1{N} \left ( \int_0^1 \frac{cos^2(\frac{\pi x}2)}{\frac{\pi^2}{8} (1 -  x^2)} dx   - \mu^2 \right )\\
   % &\approx 1,98.10^{-5}\\
 \end{align*}

 On voit donc qu'en faisant le choix $\tilde{q}=gq$, on obtient une variance nulle. C'est surprenant mais en fait c'est attendu,
 car l'intuition du changement de probabilité est d'échantillonner notre variable aléatoire selon
 une densité proche de $gq$. Le meilleur choix est donc sans surprise $gq$. 
 Seulement, en pratique, on ne peut pas faire ce choix, car pour échantilloner selon $gq$,
il faut être capable de calculer l'intégrale de $gq$ or c'est exactement ce que l'on 
cherche et que l'on suppose difficile à calculer. Dans la pratique, on choisit donc une
approximation de $gq$, un développement limité par exemple. Maintenant une autre question se
pose. Comment tirer une variable aléatoire ayant une loi à densité $\tilde{q}$? 

\subsubsection{Méthode qu rejet}
 On voudrait simuler une variable aléatoire $X$
 selon une densité $f$.
 On suppose qu'il existe une densité $g$ telle que le rapport $\frac{f}{g}$ soit borné, disons, par $C$ et qu'on 
 sait simuler $Y$ selon $g$.

 La méthode du rejet consiste à :
 \begin{itemize}
   \item tirer $Y$ selon $g$ et $U$ uniforme sur $[0,1]$,
ce tant que $U>\frac{f(Y)}{Cg(Y)}$. 
   \item Des que ce n'est plus le cas (i.e. $U\leq \frac{f(Y)}{Cg(Y)}$ ), on pose $Z=Y$.
\end{itemize}
On peut prouver que $Z$ ainsi défini est une variable aléatoire à densité $f$.
On appelle probabilité d'acceptation le nombre :
\[P_{a}(C)=P(U\leq \frac{f(Y)}{Cg(Y)})\]
\subsubsection{Simulations de $\tilde{\mu}_{N}$}

Pour simuler $\mu$ grâce à $\tilde{\mu}$, on génère par méthode du rejet un échantillon de $N$
réalisation de $X$ selon la loi à densité $\tilde{q} = \frac{\pi^2}{8}(1 - x^2)  $. On a choisi cette
fonction a partir de $g(x)= \cos (\frac{\pi x}{2})$  en prenant un développement
limité en zero que l'on a translaté et renormalisé pour obtenir une densité à 
support dans $[0,1]$.




On génère un toujours l'échantillon $(X_i)_{1\leq i \leq N}$ suivant maintenant la loi uniforme de densité q. On génère ensuite Z par la méthode du rejet, et l'on peut ainsi estimer µ comme suit: 
$$ \hat{\mu}_N = \frac1{N} \sum_{i=1}^N g(X_i)\frac{q(X_i)}{\Tilde{q}(X_i)}\ \overset{p.s.}{\longrightarrow}\ \mu $$

- Chercher une bonne FI (idée: DL au voisinage de 0)  \\
Pour choisir $\Tilde{q}$, on approxime la fonction g par son développement à l'ordre 2 en 0. Rappelons le développement limité en 0 de la fonction cosinus: 
$ cos(x) = 1 - \frac{x^2}{2} + o(x^2) $.
On prend donc pour Fonction d'Importance: 
$$\Tilde{q}_1(x):= 1 - \frac{\pi^2}{8} x^2 $$
Il est important de prendre en compte le fait que $\Tilde{q}$ ne doit pas s'annuler si gq ne s'annule pas. Le DL doit donc être corrigé pour que $\Tilde{q}$ ne devienne pas négatif en $x=1$. En $x=1$, la fonction précédente vaut $\delta = 1 - \frac{\pi^2}{8} < 0$, donc on soustrait $\delta$ à $\Tilde{q}_1$ pour obtenir notre nouvelle fonction d'importance :
$$\Tilde{q}_2(x):= \frac{\pi^2}{8}(1 - x^2) $$

- Calcul de la variance théorique


- Calcul de la probabilité d'acceptation théorique:
$$P_a = \frac1C \int_0^1 g(x)q(x)dx 
= \int_0^1 cos(\frac{\pi x}{2})dx 
=\frac{2}{\pi} 
\approx 0,637$$

\textbf{Application numérique (TP):}\\
Les Figures~\ref{TP1_IS_DL} et~\ref{TP1_IS_TCL} nous montrent l'étude des deux FI $\Tilde{q}_1$ et $\Tilde{q}_2$ définies précédemment.

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP1/DL_ordre2_g.png}
\includegraphics[width=0.4\textwidth]{TP1/DL_ordre2_corrige.png}
\caption{représentation des fonctions gq (en orange) et $\Tilde{q}$ (en bleu): à gauche $\Tilde{q}_1$, à droite $\Tilde{q}_2$}
\label{TP1_IS_DL}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP1/IS_TCL_DL_ordre2.png}
\includegraphics[width=0.4\textwidth]{TP1/IS_TCL_DL_ordre2_corrige.png}
\caption{TCL appliqués respectivement à $\Tilde{q}_1$ (à gauche) et $\Tilde{q}_2$ (à droite)}
\label{TP1_IS_TCL}
\end{figure}

Les résultats du TCL appliqués aux échantillons produits mettent ici en valeur l'importance d'avoir une fonction d'importance du même signe que gq, mais aussi que notre développement limité en 0 corrigé de gq $\Tilde{q}_2$  semble être une bonne approximation pour notre problème. On obtient en effet une variance empirique $Var(\hat{\mu}_N) = 2,11.10^{-5} $ et une probabilité d'acceptation $P_a = 0,669$.



\subsection{Conclusion}
On cherche maintenant à comparer les deux méthodes précédemment appliquées.

- Comparer le rapport des variances des 2 méthodes (théoriquement et par simulations)\\
$$\frac{\textrm{variance théorique (Monté Carlo)}}{\textrm{variance théorique Importance Sampling}} = \frac{1,89.10^{-3}}{1,97.10^{-5}} \approx 95,6 $$
$$\frac{\textrm{variance empirique (Monté Carlo)}}{\textrm{variance empirique (Importance Sampling)}} = \frac{1,84.10^{-3}}{2,11.10^{-5}} \approx 83,7 $$
On en conclue donc que la méthode d'Importance Sampling est bien meilleure.\\

- Valider par simulations les TCL pour les 2 méthodes en comparant la loi théorique (loi normale) à la loi empirique (histogramme)\\
Les figures~\ref{TP1_MC_TCL} et~\ref{TP1_IS_TCL} nous prouvent que la distribution de l'erreur faite par nos estimateurs suivent une gaussienne centrée réduite, donc valident nos approximations de µ.\\

- Comparer les budgets pour chaque méthode\\
\textbf{TO DO}\\

- Calculer la variance de l’estimateur en prenant la FI optimale\\
Lorsqu'on choisit $\Tilde{q}=gq/ \int g(x)q(x)dx$, on obtient une variance empirique nulle pour la méthode d'Importance Sampling et une estimation de la probabilité d'acceptation d'environ 0,64. La variance théorique vaut quant à elle $P_a(theo)=-2,22.10^{-18}\approx 0$ (erreur d'arrondi de l'ordinateur) et la Probabilité d'acceptation théorique $=0.637$.\\










\newpage
\section{Filtre de Kalman: Poursuite d'un mobile en mouvement rectiligne uniforme bruité}
\subsection{Introduction}
Dans ce TP, nous allons simuler la trajectoire d'un mobile, et tenter de retrouver ces valeurs grâce à des observations que nous aurons de sa trajectoire.

On considère l'état à l'instant t $X_t$ où t varie entre 0 et T, et sa mesure $Y_t$
\[ X_t = \left (
   \begin{array}{c}
      x_t \\
      y_t \\
      \Dot{x_t} \\
      \Dot{y_t} \\
   \end{array} \right )
   ,\ Y_t = \left (
   \begin{array}{c}
      x_t \\
      y_t \\
   \end{array} \right )
   ,\ W_t = \left (
   \begin{array}{c}
      w_t^1 \\
      w_t^2 \\
   \end{array} \right )
\]

équation de la dynamique : 
\[ m \left (
   \begin{array}{c}
      \ddot{x_t} \\
      \ddot{y_t} \\
   \end{array} \right )
   = \left (
   \begin{array}{c}
      w_k^1 \\
      w_k^2 \\
   \end{array} \right )\ 
   \forall t \in [t_k, t_{k+1}]
\]

On fixe $m=1$ et on obtient l'équation d'état suivante :
\[ X_n = \left ( 
   \begin{array}{cccc} %Fn
      1 & 0 & \delta t & 0 \\
      0 & 1 & 0 & \delta t \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1 \\
   \end{array} \right )
   \left ( 
   \begin{array}{cccc} %Xn-1
      x_{t_{n-1}} \\
      y_{t_{n-1}} \\
      \Dot{x}_{t_{n-1}} \\
      \Dot{y}_{t_{n-1}} \\
   \end{array} \right )
   + \left (
   \begin{array}{cc}
      \frac{c(\delta t)^2}{2m} & 0 \\
      0 & \frac{c(\delta t)^2}{2m} \\
      \frac{c\delta t}{m} & 0 \\
      0 & \frac{c\delta t}{m} \\
   \end{array} \right )
   \left (
   \begin{array}{c}
      w_n^1 \\
      w_n^2 \\
   \end{array} \right )
   = F X_{n-1} + G W_n
\]

Condition initiale: 
\[ m_0 = \mathbb{E}[X_0] = \left (
   \begin{array}{c}
      5000\ m \\
      5000\ m \\
      -20\ m/s \\
      20\ m/s \\
   \end{array} \right )
   ,\ P_0 = \left (
   \begin{array}{cccc}
      (2000\ m)^2 & 0 & 0 & 0 \\
      0 & (2000\ m)^2 & 0 & 0 \\
      0 & 0 & (5\ m/s)^2 & 0 \\
      0 & 0 & 0 & (5\ m/s)^2 \\
   \end{array} \right )
\]

Constantes du problème:
$$ \left\{
   \begin{array}{l}
      \delta t = 1\ seconde\\
      T = N \delta t = 200\ s \\
      c = 2\ m/s^2 \\
   \end{array} \right .$$

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP2/position_réelle.png}
\includegraphics[width=0.4\textwidth]{TP2/vitesse_reelle.png}
\caption{Trajectoire et vitesse associée, que l'on cherche à retrouver}
\label{TP2_pos_vit}
\end{figure}

\subsection{Titre intermédiaire}
\subsection{Conclusion}


\newpage

\section{TP3 : Borne de Cramer Rao.}
\subsection{Introduction}
\subsubsection{Formulation du problème}

Dans ce TP, on se propose de calculer une borne de Cramer Rao pour un problème de filtrage. Le cadre général est le suivant.
On se donne un système  $(X_{k})_{k\geq 0}$ sujet à une certaine dynamique,
ainsi qu'une série de mesures bruitées $(y_{k})_{k \geq 0}$ (bruit gaussien de variance $\sigma_{k}^{2}$) : 
\[\left\{\begin{array}{ll}
   X_{k+1} = \phi(k,k+1)X_{k} \\
   y_{k}=h_{k}(X_{k})+\epsilon_{k}
\end{array}\right. \]

Avec $X_{0} \sim  \mathcal{N} (X_{\nu},P_{0})$. On peut montrer que la matrice d'information ralative à l'instant $k$ vérifie
la relation de récurence suivante : 

\[ J_{k} = \frac{1}{\sigma_{k}}\left(\frac{\partial h_{k}}{\partial X_{k}}\right)\left(\frac{\partial h_{k}}{\partial X_{k}}\right)^{T}+\left(\frac{\partial X_{k-1}}{\partial X_{k}}\right)^{T}J_{k-1}\left(\frac{\partial X_{k-1}}{\partial X_{k}}\right)\]

Avec $J_{0}=P_{0}^{-1}$ et $\phi(k,k+1)=\frac{\partial X_{k+1}}{\partial X_{k}}$.
La borne de Cramer Rao à l'instant $k$ est alors égale à l'inverse de la matrice d'information : 
\[BCR_{k}= J_{k}^{-1}\]

Nous étudions le système suivant : un bateau émetteur de bruit se déplace dans le plan de manière rectiligne uniforme. 
Un bateau observateur mesure la direction d'ou lui parvient le bruit, plus précisément, il mesure l'angle $\theta_{k}$ que fait cette direction avec l'horizontale, ce toute les secondes pendant 100s. Le bateau observateur se déplace également en ligne droite à vitesse constante.
L'état de l'émeteur est donné par sa position et sa vitesse : $X_{k} = (x_{k},y_{k},\dot{x}_{k},\dot{y}_{k})$, sa matrice de transition est :
\[\phi(k,k+1) = \phi =  \begin{pmatrix}
  1 & 0 & T & 0 \\
  0 & 1 & 0 & T \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 
  \end{pmatrix}\]

Pour pouvoir discriminer entre différentes
trajectoires possibles, le bateau doit virer de bord à un certain moment. Si ça n'est pas le cas, on peut vérifier que des trajectoires différentes peuvent donner lieu à des mesures d'angle identiques.
Pour simplifier l'étude, on suppose que le bateau vire de bord toujours au même moment : au milieu du trajet. Le bateau observateur choisit l'angle $\varphi $ duquel il tourne. Chaque choix de $\varphi$ 
conduit à une précision différente de l'estimation de la trajectoire  de l'émeteur.
La question est donc la suivante : quel choix de $\varphi$ est le meilleur pour estimer la trajectoire de l'émeteur?
\\
Pour répondre à cette question, nous allons simuler la dynamique de l'éméteur et de l'observateur. Grâce à ces dynamiques, nous allons
calculer le gradient de la fonction d'observation et nous allons en déduire la matrice d'information.
Dans notre cas, la fonction d'observation est l'angle que fait la direction d'observation avec l'horizontale. 
En terme de l'abscisse et de l'ordonnée de l'éméteur et de l'observateur, cette fonction s'écrit : 
\[\theta_{k} = h_{k}(X_{k})=\arctan \left(\frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}\right)\]

En inversant la matrice d'observation, on obtiendra la borne de Cramer Rao, sous forme d'une matrice carrée d'ordre 4.
On cherchera ensuite la valeur de $\varphi$ qui minimise le critère : $\sqrt{\sigma_{x_{n}}^{2}+\sigma_{y_{n}}^{2}}$

\subsubsection{Calcul du gradient de $h_{k}$ par rapport à $X_{k}$}
Déjà, on voit facilement que : 
\[\frac{\partial h_{k}}{\partial \dot{x}_{k}} = 0 \] et :
\[\frac{\partial h_{k}}{\partial \dot{y}_{k}} = 0 \]
Ensuite, par le calcul, on a que : 
\[\frac{\partial h_{k}}{\partial x_{k}}=\frac{y_{k}^{o}-y_{k}}{\left(x_{k}-x_{k}^{o}\right)^{2}\left(1+\left(\frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}\right)^{2}\right)}\]
et :

\[\frac{\partial h_{k}}{\partial y_{k}}= \frac{1}{\left(x_{k}-x_{k}^{o}\right)\left(1+\left( \frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}  \right)^{2}\right)}\]
\subsection{Simulations des dynamiques de l'observateur et de l'émetteur}
Nous avons simulé la dynamique du système pour différents changements de cap de l'observateur, avec les paramètres du tableau \ref{paramètres}.
 Nous obtenons les tracés de la figure \ref{trajectoire}


\begin{figure}[h!]
  \centering
  \caption{Paramètres pour la simulation}
  \label{paramètres}
  \begin{tabular}{|*{11}{c|}}
    \hline Horizon temporel & $T = 1$ \\
    \hline Nombre de mesures  & $N=100$ \\
    \hline Ecart type du bruit & $\sigma_{k}=1^{\circ}$\\
    \hline Vitesse de l'observateur & $V_{o}=10 m.s^{-2}$\\
    \hline Position initiale de l'observateur &  $(x_{0}^{o},y_{0}^{o})=(0,0)$ m\\
    \hline Vitesse de l'émetteur & $V_{e} = 5 m.s^{-2}$\\
    \hline Cap de l'émetteur & $\alpha =-20^{\circ}$\\
    \hline Position initiale de l'émetteur & $(x_{0},y_{0})=(2000,2000)$\\
    \hline Incertitude initiale sur la position de l'émetteur & $P_{0}=diag(1000,1000,10,10)^{2}$\\
    \hline
  \end{tabular}
\end{figure}


 \begin{figure}[h!]
  \centering
  \includegraphics[width = 15cm]{TP3/trajectoires.png}
  \label{trajectoire}
  \caption{Trajectoires de l'émetteur et de l'observateur pour différentes valeurs de l'angle $\varphi$}
\end{figure}

Grâce à ces dynamiques, nous sommes capables de calculer la Borne de Cramer Rao associée à cet estimateur de la position de l'émetteur. 
Nous présentons ces résultat en figure \ref{cramerrao}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width = 15cm]{TP3/cramer rao.png}
  \caption{Borne de Cramer Rao pour l'abscisse et l'ordonnée de la position de l'émetteur au cours du temps, pour différentes valeurs de $\varphi$}
  \label{cramerrao}
\end{figure}

\subsection{Optimisation de la valeur de $\varphi$}
Pour trouver la valeur de $\varphi$ optimale, nous avons simplement testé toutes les valeurs entre $0^{\circ}$ et $360^{\circ}$, avec un pas de 
$1^{\circ}$. Nous avons trouvé une valeur optimale de $\varphi = 109^{\circ}$, ce qui correspond à la trajectoire représentée en figure
\ref{optimale}. La borne de Cramer Rao associée est représentée en figure \ref{crameropti}. Le critère associé est un écart type sur la position de 128m.
\begin{figure}
  \centering
  \includegraphics[width = 10 cm]{TP3/trajopti.png}
  \label{optimale}
  \caption{Trajectoire optimale pour l'observateur}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width = 10 cm]{TP3/crameropti.png}
  \label{crameropti}
  \caption{Borne de Cramer Rao pour la trajectoire optimale}
\end{figure}

\subsection{Conclusion}
En conclusion, ce TP nous aura permis de voir comment la Borne de Cramer Rao permet de réaliser le choix d'une stratégie pour 
optimiser la précision sur l'estimation de état d'un système, grâce à la seule connaissance d'un modèle à priori.

\newpage

\section{TP4: titre du TP}
\subsection{Introduction}

\subsection{Titre intermédiaire}

\subsection{Conclusion}


\end{document}
