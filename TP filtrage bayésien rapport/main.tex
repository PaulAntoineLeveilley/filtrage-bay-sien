\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin=2.5cm}
\usepackage{mathtools} %\usepackage{align}
%\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{bbold}

\title{SOD333 - Rapport}
\author{Paul-Antoine Leveilley \& Mila Rocco}
\date{Septembre 2022}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage







\newpage
\section{TP1: Calcul d'une intégrale par méthode de Monte-Carlo, échantillonage préférentiel.}
\subsection{Introduction}
Dans ce TP, on se propose de calculer une intégrale par méthode de Monte Carlo. De manière générale, la méthode de Monte Carlo
consisite à approcher l'intégrale : 
\[\mu = \int g(x)q(x)dx\] 
Ou $q$ est une densité, par l'estimateur : 
\begin{equation}
  \label{estim}
  \hat{\mu}_N = \frac{1}{N} \sum_{i=1}^N g(X_i)
\end{equation}
En effet, d'après la loi forte des grands nombres, 
\[\hat{\mu}_N\underset{p.s.}{\longrightarrow}\mu \] 

Pour illustrer cette méthode, nous allons estimer l'intégrale 
\[\int_0^1 cos(\frac{\pi x}{2})dx\]
Dont la valeur se calcule analytiquement, et vaut $\frac{2}{\pi}$ \\
Dans un premier temps, on mettra en oeuvre la méthode dite "naive" de Monte Carlo. Des considérations de minimisation de
la variance de l'estimateur nous conduirons à employer une méthode
plus précise. Ce sera l'objet de la deuxième partie.\\ 

 \subsection{Calcul par méthode de Monte Carlo naive}
Une première idée est de choisir la loi uniforme sur $[0,1]$, qui admet pour densité $q(x) = \mathbb{1}_{x \in [0,1]}$, ainsi que $g(x) = \cos (\frac{\pi x}{2})$
\subsubsection{Calcul préliminaire : variance de l'estimateur naif.}
Comme expliqué précédemment, la variance de l'estimation 
est un indicateur clé pour nous. On cherche à ce qu'elle soit la plus petite possible, 
de sorte à avoir une précision donnée sur le calcul de l'intégrale, en un nombre
minimum d'itérations. Employons nous donc à calculer cette variance pour (\ref{estim}). 
\begin{align*} 
   Var(\hat{\mu}_N) &= \frac1{N^2} \sum_{i=1}^N Var(g(X_i))\\ 
   &= \frac1{N^2}\times N Var(cos(\frac{\pi X}2)) \\ 
   &= \frac1{N} \left ( \int_0^1 cos^2(\frac{\pi x}2)dx - \left ( \int_0^1 cos(\frac{\pi x}2)dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{1+cos(\pi x)}{2}dx - 
    \frac4{\pi^2} \right )\\
   &= \frac1{N} \left ( \frac{1}{2} - 
    \frac4{\pi^2} \right )\\
 \end{align*}
\subsubsection{Simulations}
Dans l'intégralité de ce TP, on prendra $N=50$.
Pour réaliser cette estimation de $\mu$, nous simulons en machine
le tirage d'un échantillon $(X_{0},\textrm{...},X_{N})$ iid de loi uniforme sur $[0,1]$.
Ensuite, nous calculons une estimation de $\mu$ en utilisant l'estimateur (\ref{estim})
Nous réalisons ce calcul 10 fois, avec des échantillons différents. Le tableau (\ref{naif}) 
présente les résultat.
\begin{figure}[h!]
  \centering
  \caption{Calcul de l'estimation naive de $\mu$ pour 10 échantillons différents}
  \label{naif}
  \begin{tabular}{|*{11}{c|}}
    \hline 0.6685 & 0.67890& 0.69391& 0.59470& 0.69625&0.6424 & 0.64359& 0.61404& 0.65725& 0.63069 \\
    \hline
  \end{tabular}
\end{figure}

Nous observons que le résultat est assez proche de la valeur exacte de l'intégrale 
($\frac{2}{\pi} \approx 0.6366$). Cela dit, on constate également une forte variance des résultats de l'estimation. Nous avons cherché à 
estimer cette variance pour un nombre de $1000$ réalisations de l'estimation $\hat{\mu}_{N}$. Nous obtenons une variance empirique de $Var_{emp}(\hat{\mu}_{N})=0.00184$.
Ceci est conforme à ce qui est attendu. Le calcul de la partie précédente prédit en effet :  $Var(\hat{\mu}_{N}) \approx 1,89.10^{-3})$


\subsubsection{Théorème central limite : illustration}

Le théorème centrale limite prédit que l'erreur normalisée doit se comporter commme une loi 
normale centrée réduite pour $N \rightarrow \infty$. C'est à dire : 
\[\frac {\hat{\mu}_{N}-\mu }{\sigma /{\sqrt {N}}}\underset{\mathcal{L} }{\longrightarrow}\mathcal{N} (0,1)\]

Pour illustrer ceci, nous estimons la distribution de l'erreur renormalisée
par l'histogramme de 1000 réalisations de l'erreur renormalisée empirique. Nous présentons
ce résultat sur le graphique de la figure \ref{TP1_MC_TCL}, en superposant l'histogramme avec la représentation graphique de la densité de la gaussienne centrée réduite.
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP1/MC_brute_TCL.png}
\caption{Distribution empirique de l'erreur renormalisée et densité de la gaussienne centrée réduite}
\label{TP1_MC_TCL}
\end{figure}

Nous constatons que, comme le prédit le TCL, l'erreur renormalisée possède une distribution proche d'une gaussienne centrée réduite.



\subsection{Echantillonage pondéré et réduction de la variance}
L'estimation de la partie précédente conduit à une grande variance du résultat.
Ce n'est pas satisfaisant et l'on peut faire mieux. En effet, quand nous estimons 
$\mu$ avec (\ref{estim}), nous donnons autant d'importance a chaque réalisation
de $X_{i}$ dans l'intégrale. Mais certains termes ont une contribution plus
imoprtante dans le calcul. Ce sont les termes pour lesquels
$g(X_{i})$ est grand. Intuitivement, nous devons donc chercher à estimer "finement" les grandes valeurs de $gq$. Il faut donc 
tirer selon une densité la plus "proportionelle" possible à $gq$. C'est l'idée de l'échantillonage pondéré (préférentiel).\\

L'échantillonage pondéré consisite à remplacer $\hat{\mu}_{N}$ par : 
\[\tilde{\mu}_{N}= \frac1{N} \sum_{i=1}^N g(X_i)\frac{q(X_i)}{\Tilde{q}(X_i)}\]
Ou, $\tilde{q}$ est une densite et $(X_{i})_{1\leq i \leq n }$ est distribué selon une loi de densité $\tilde{q}$ 


En effet, dans ce cas, par loi forte des grands nombres, $\tilde{\mu}_{N}$ converge presque surement vers : 
\[\mathbb{E}^{\mathbb{Q}}[g(X)\frac{q(X)}{\tilde{q}(X)}]=\int_0^1 \frac{g(x)q(x)}{\Tilde{q}(x)} \Tilde{q}(x) dx = \int_0^1 g(x)q(x)dx = \mu \]
 Ou $\mathbb{E}^{\mathbb{Q}}$ désigne l'espérence sous la nouvelle probabilité $\mathbb{Q}$. Pour cette raison, on 
 appelle également cette technique "changement" de probabilité.

Maintenant, puisque c'est cela que nous cherchons à optimiser, calculons la variance de ce nouvel estimateur.

\begin{align*} 
   Var(\tilde{\mu}_N) &= \frac1{N^2} \sum_{i=1}^N Var \left( g(X_i)\frac{q(X_i)}{\Tilde{q}(X_i)} \right)\\ 
   &= \frac1{N} \left ( \int_0^1 \left (\frac{g(x)q(x)}{\Tilde{q}(x)} \right )^2 \Tilde{q}(x) dx  - \left ( \int_0^1 \frac{g(x)q(x)}{\Tilde{q}(x)} \Tilde{q}(x) dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{g^2(x)q^2(x)}{\Tilde{q}(x)} dx  - \left ( \int_0^1 g(x)q(x) dx \right )^2 \right )\\
   &= \frac1{N} \left ( \int_0^1 \frac{g^2(x)q^2(x)}{\Tilde{q}(x)} dx  - \mu^{2} \right )\\
   % &= \frac1{N} \left ( \int_0^1 \frac{cos^2(\frac{\pi x}2)}{\frac{\pi^2}{8} (1 -  x^2)} dx   - \mu^2 \right )\\
   % &\approx 1,98.10^{-5}\\
 \end{align*}
 On voit donc qu'en faisant le choix $\Tilde{q}=gq/ \int g(x)q(x)dx$, on obtient une variance nulle. C'est surprenant mais en fait c'est attendu,
 car l'intuition du changement de probabilité est d'échantillonner notre variable aléatoire selon
 une densité la plus "proportionelle" à $gq$. Le meilleur choix est donc sans surprise celui la. 
 Seulement, en pratique, on ne peut pas faire ce choix, car pour échantilloner selon $gq/\int gq$,
il faut être capable de calculer l'intégrale de $gq$ or c'est exactement ce que l'on 
cherche et que l'on suppose difficile à calculer. Dans la pratique, on choisit donc une
approximation de $gq/ \int gq$, un développement limité par exemple. Maintenant une autre question se
pose. Comment tirer une variable aléatoire ayant une loi à densité $\tilde{q}$? 

\subsubsection{Méthode qu rejet}
 On voudrait simuler une variable aléatoire $X$
 selon une densité $f$.
 On suppose qu'il existe une densité $g$ telle que le rapport $\frac{f}{g}$ soit borné, disons, par $C$ et qu'on 
 sait simuler $Y$ selon $g$.

 La méthode du rejet consiste à :
 \begin{itemize}
   \item tirer $Y$ selon $g$ et $U$ uniforme sur $[0,1]$,
ce tant que $U>\frac{f(Y)}{Cg(Y)}$. 
   \item Des que ce n'est plus le cas (i.e. $U\leq \frac{f(Y)}{Cg(Y)}$ ), on pose $Z=Y$.
\end{itemize}
On peut prouver que $Z$ ainsi défini est une variable aléatoire à densité $f$.
On appelle probabilité d'acceptation le nombre :
\[P_{a}(C)=P(U\leq \frac{f(Y)}{Cg(Y)})\]
\subsubsection{Simulations de $\tilde{\mu}_{N}$}

Pour simuler $\mu$ grâce à $\tilde{\mu}$, on génère par méthode du rejet un échantillon de $N$
réalisation de $X$ selon la loi à densité $\tilde{q} = \frac{3}{2}(1 - x^2)  $. On a choisi cette
fonction a partir de $g(x)= \cos (\frac{\pi x}{2})$  en prenant un développement
limité en zero que l'on a translaté et renormalisé pour obtenir une densité à 
support dans $[0,1]$.

Cette fois ci, nous obtenons la valeur $\tilde{\mu}_{N}=0.63582$. Nous constatons une variance moindre. 

\begin{figure}
   \centering
   \caption{Représentation graphique de $g$ et de $\tilde{q}$}
   \includegraphics[width = 10cm]{TP1/gqtilde.png}
\end{figure}

Comme dans la partie précédente, nous estimons empiriquement la variance de $\tilde{\mu}_{N}$. Nous obtenons
$Var_{emp}(\tilde{\mu}_{N})=1.93.10^{-5}$, pour une valeur théorique de $Var_{\tilde{\mu}_{N}}\approx 1.98.10^{-5}$

Nous avons réalisé le même travail de comparaison de l'erreur renormalisée avec
la loi normale centrée réduite. Nous présentons cela en figure \ref{TCL_imp}. 

\begin{figure}[h!]
   \centering
   \label{TCL_imp}
   \caption{Comparaison de la densité de la loi $\mathcal{N}(0,1)$ et de l'erreur renormalisée de $\tilde{\mu}_{N}$ }
   \includegraphics[width = 10 cm]{TP1/TCL_importance_sampling.png}
\end{figure}

Nous nous sommes intéressé à la probabilité d'acceptation dans notre méthode du rejet. En effet, la complexité de notre algorithme d'estimation dépend de cette probabilité.
On peut montrer qu'elle vaut
$P_{a}=\frac{1}{C}=\frac{1}{\tilde{q}(0)}= \frac{2}{3}$ dans notre cas. Par le calcul, nous obtenons une probabilité d'acceptation empirique de $0,667$, ce qui est attendu. Cela signifie qu'en moyenne, pour simuler 2 échantillons de $X$ selon
$\tilde{q}$, il nous faut de l'ordre de 3 opérations. 





\subsection{Conclusion}


Pour conclure, nous avons testé deux méthodes pour calcule notre intégrale par méthode de Monte Carlo. Une méthode dite naïve
qui donne lieu à une grande variance des résultat puis, une méthode plus rafinée qui fait
appel à un changement de probabilité. Grâce à cette méthode, nous avons pu réduire la variance d'un facteur presque 100 (83,7 pour être précis.) C'est donc très satisfaisant. Comparons le budget des deux méthodes. Pour rappel, nous avons
$N=50$. 
\begin{itemize}
   \item Méthode, naïve : pour obtenir une estimation, nous réalisons de l'ordre de N opération puisqu'il s'agit simplement de tirer $N$ réalisations d'une variable aléatoire et d'en faire la moyenne.
   \item Importance sampling : Cette fois ci, pour obtenir une estimation de $\mu$, nous avons besoin de N réalisations de $X$ selon $\tilde{q}$ mais pour tirer $X$ comme ceci, on fait en moyenne $\frac{3}{2}$ essais (inverse de la probabilité d'acceptation). In fine, on fait de l'ordre de $\frac{3}{2}N$ opérations, ce qui n'est pas beaucoup plus compte tenu du gain en variance. 
\end{itemize}

On conclut donc que l'échantillonage préférentiel est la méthode de 
choix lorsue l'on veut estimer une intégrale par méthode de Monte Carlo.
Ceci a des applications dans le calcul d'intégrales sur des espaces de grandes dimension. Cela sert en le filtrage bayésien, 
lorsque l'on propage des distributions conditionelles par exemple.

\newpage
\section{TP2 : Filtre de Kalman: Poursuite d'un mobile en mouvement rectiligne uniforme bruité}
\subsection{Introduction}
Dans ce TP, nous allons simuler la trajectoire d'un mobile, et tenter de retrouver ces valeurs grâce à des observations que nous aurons de sa trajectoire.

On considère l'état à l'instant t $X_t$ où t varie entre 0 et T, et sa mesure $Y_t$
\[ X_t = \left (
   \begin{array}{c}
      x_t \\
      y_t \\
      \Dot{x_t} \\
      \Dot{y_t} \\
   \end{array} \right )
   ,\ Y_t = \left (
   \begin{array}{c}
      x_t \\
      y_t \\
   \end{array} \right )
   ,\ W_t = \left (
   \begin{array}{c}
      w_t^1 \\
      w_t^2 \\
   \end{array} \right )
\]

équation de la dynamique : 
\[ m \left (
   \begin{array}{c}
      \ddot{x_t} \\
      \ddot{y_t} \\
   \end{array} \right )
   = \left (
   \begin{array}{c}
      w_k^1 \\
      w_k^2 \\
   \end{array} \right )\ 
   \forall t \in [t_k, t_{k+1}]
\]

On fixe $m=1$ et on obtient l'équation d'état suivante :
\[ X_n = \left ( 
   \begin{array}{cccc} %Fn
      1 & 0 & \delta t & 0 \\
      0 & 1 & 0 & \delta t \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1 \\
   \end{array} \right )
   \left ( 
   \begin{array}{cccc} %Xn-1
      x_{t_{n-1}} \\
      y_{t_{n-1}} \\
      \Dot{x}_{t_{n-1}} \\
      \Dot{y}_{t_{n-1}} \\
   \end{array} \right )
   + \left (
   \begin{array}{cc}
      \frac{c(\delta t)^2}{2m} & 0 \\
      0 & \frac{c(\delta t)^2}{2m} \\
      \frac{c\delta t}{m} & 0 \\
      0 & \frac{c\delta t}{m} \\
   \end{array} \right )
   \left (
   \begin{array}{c}
      w_n^1 \\
      w_n^2 \\
   \end{array} \right )
   = F X_{n-1} + G W_n
\]

Condition initiale: 
\[ m_0 = \mathbb{E}[X_0] = \left (
   \begin{array}{c}
      5000\ m \\
      5000\ m \\
      -20\ m/s \\
      20\ m/s \\
   \end{array} \right )
   ,\ P_0 = \left (
   \begin{array}{cccc}
      (2000\ m)^2 & 0 & 0 & 0 \\
      0 & (2000\ m)^2 & 0 & 0 \\
      0 & 0 & (5\ m/s)^2 & 0 \\
      0 & 0 & 0 & (5\ m/s)^2 \\
   \end{array} \right )
\]

Constantes du problème:
$$ \left\{
   \begin{array}{l}
      \delta t = 1\ seconde\\
      T = N \delta t = 200\ s \\
      c = 2\ m/s^2 \\
   \end{array} \right .$$

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{TP2/position_réelle.png}
\includegraphics[width=0.4\textwidth]{TP2/vitesse_reelle.png}
\caption{Trajectoire et vitesse associée, que l'on cherche à retrouver}
\label{TP2_pos_vit}
\end{figure}

\subsection{Titre intermédiaire}
\subsection{Conclusion}


\newpage

\section{TP3 : Borne de Cramer Rao.}
\subsection{Introduction}
\subsubsection{Borne de Cramer Rao pour un problème de filtrage}

Dans ce TP, on se propose de calculer une borne de Cramer Rao pour un problème de filtrage. Le cadre général est le suivant.
On se donne un système  $(X_{k})_{k\geq 0}$ sujet à une certaine dynamique,
ainsi qu'une série de mesures bruitées $(y_{k})_{k \geq 0}$ (bruit gaussien de variance $\sigma_{k}^{2}$) : 
\[\left\{\begin{array}{ll}
   X_{k+1} = \phi(k,k+1)X_{k} \\
   y_{k}=h_{k}(X_{k})+\epsilon_{k}
\end{array}\right. \]

Avec $X_{0} \sim  \mathcal{N} (X_{\nu},P_{0})$. On peut montrer que la matrice d'information ralative à l'instant $k$ vérifie
la relation de récurence suivante : 

\[ J_{k} = \frac{1}{\sigma_{k}}\left(\frac{\partial h_{k}}{\partial X_{k}}\right)\left(\frac{\partial h_{k}}{\partial X_{k}}\right)^{T}+\left(\frac{\partial X_{k-1}}{\partial X_{k}}\right)^{T}J_{k-1}\left(\frac{\partial X_{k-1}}{\partial X_{k}}\right)\]

Avec $J_{0}=P_{0}^{-1}$ et $\phi(k,k+1)=\frac{\partial X_{k+1}}{\partial X_{k}}$.
La borne de Cramer Rao à l'instant $k$ est alors égale à l'inverse de la matrice d'information : 
\[BCR_{k}= J_{k}^{-1}\]

\subsubsection{Application : poursuite de bateau}
Nous étudions le système suivant : un bateau émetteur de bruit se déplace dans le plan de manière rectiligne uniforme. 
Un bateau observateur mesure la direction d'ou lui parvient le bruit, plus précisément, il mesure l'angle $\theta_{k}$ que fait cette direction avec l'horizontale, ce toute les secondes pendant 100s. Le bateau observateur se déplace également en ligne droite à vitesse constante.
L'état de l'émeteur est donné par sa position et sa vitesse : $X_{k} = (x_{k},y_{k},\dot{x}_{k},\dot{y}_{k})$, sa matrice de transition est :
\[\phi(k,k+1) = \phi =  \begin{pmatrix}
  1 & 0 & T & 0 \\
  0 & 1 & 0 & T \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 
  \end{pmatrix}\]

Pour pouvoir discriminer entre différentes
trajectoires possibles, le bateau doit virer de bord à un certain moment. Si ça n'est pas le cas, on peut vérifier que des trajectoires différentes peuvent donner lieu à des mesures d'angle identiques.
Pour simplifier l'étude, on suppose que le bateau vire de bord toujours au même moment : au milieu du trajet. Le bateau observateur choisit l'angle $\varphi $ duquel il tourne. Chaque choix de $\varphi$ 
conduit à une précision différente de l'estimation de la trajectoire  de l'émeteur.
La question est donc la suivante : quel choix de $\varphi$ est le meilleur pour estimer la trajectoire de l'émeteur?
\\
Pour répondre à cette question, nous allons simuler la dynamique de l'éméteur et de l'observateur. Grâce à ces dynamiques, nous allons
calculer le gradient de la fonction d'observation et nous allons en déduire la matrice d'information.
Dans notre cas, la fonction d'observation est l'angle que fait la direction d'observation avec l'horizontale. 
En terme de l'abscisse et de l'ordonnée de l'éméteur et de l'observateur, cette fonction s'écrit : 
\[\theta_{k} = h_{k}(X_{k})=\arctan \left(\frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}\right)\]

En inversant la matrice d'observation, on obtiendra la borne de Cramer Rao, sous forme d'une matrice carrée d'ordre 4.
On cherchera ensuite la valeur de $\varphi$ qui minimise le critère : $\sqrt{\sigma_{x_{n}}^{2}+\sigma_{y_{n}}^{2}}$

\subsubsection{Calcul du gradient de $h_{k}$ par rapport à $X_{k}$}
Déjà, on voit facilement que : 
\[\frac{\partial h_{k}}{\partial \dot{x}_{k}} = 0 \] et :
\[\frac{\partial h_{k}}{\partial \dot{y}_{k}} = 0 \]
Ensuite, par le calcul, on a que : 
\[\frac{\partial h_{k}}{\partial x_{k}}=\frac{y_{k}^{o}-y_{k}}{\left(x_{k}-x_{k}^{o}\right)^{2}\left(1+\left(\frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}\right)^{2}\right)}\]
et :

\[\frac{\partial h_{k}}{\partial y_{k}}= \frac{1}{\left(x_{k}-x_{k}^{o}\right)\left(1+\left( \frac{y_{k}-y_{k}^{o}}{x_{k}-x_{k}^{o}}  \right)^{2}\right)}\]
\subsection{Simulations des dynamiques de l'observateur et de l'émetteur}
Nous avons simulé la dynamique du système pour différents changements de cap de l'observateur, avec les paramètres du tableau \ref{paramètres}.
 Nous obtenons les tracés de la figure \ref{trajectoire}


\begin{figure}[h!]
  \centering
  \caption{Paramètres pour la simulation}
  \label{paramètres}
  \begin{tabular}{|*{11}{c|}}
    \hline Horizon temporel & $T = 1$ \\
    \hline Nombre de mesures  & $N=100$ \\
    \hline Ecart type du bruit & $\sigma_{k}=1^{\circ}$\\
    \hline Vitesse de l'observateur & $V_{o}=10 m.s^{-2}$\\
    \hline Position initiale de l'observateur &  $(x_{0}^{o},y_{0}^{o})=(0,0)$ m\\
    \hline Vitesse de l'émetteur & $V_{e} = 5 m.s^{-2}$\\
    \hline Cap de l'émetteur & $\alpha =-20^{\circ}$\\
    \hline Position initiale de l'émetteur & $(x_{0},y_{0})=(2000,2000)$\\
    \hline Incertitude initiale sur la position de l'émetteur & $P_{0}=diag(1000,1000,10,10)^{2}$\\
    \hline
  \end{tabular}
\end{figure}


 \begin{figure}[h!]
  \centering
  \includegraphics[width = 15cm]{TP3/trajectoires.png}
  \label{trajectoire}
  \caption{Trajectoires de l'émetteur et de l'observateur pour différentes valeurs de l'angle $\varphi$}
\end{figure}

Grâce à ces dynamiques, nous sommes capables de calculer la Borne de Cramer Rao associée à cet estimateur de la position de l'émetteur. 
Nous présentons ces résultat en figure \ref{cramerrao}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width = 15cm]{TP3/cramer rao.png}
  \caption{Borne de Cramer Rao pour l'abscisse et l'ordonnée de la position de l'émetteur au cours du temps, pour différentes valeurs de $\varphi$}
  \label{cramerrao}
\end{figure}

\subsection{Optimisation de la valeur de $\varphi$}
Pour trouver la valeur de $\varphi$ optimale, nous avons simplement testé toutes les valeurs entre $0^{\circ}$ et $360^{\circ}$, avec un pas de 
$1^{\circ}$. Nous avons trouvé une valeur optimale de $\varphi = 109^{\circ}$, ce qui correspond à la trajectoire représentée en figure
\ref{optimale}. La borne de Cramer Rao associée est représentée en figure \ref{crameropti}. Le critère associé est un écart type sur la position de 128m.
\begin{figure}
  \centering
  \includegraphics[width = 10 cm]{TP3/trajopti.png}
  \label{optimale}
  \caption{Trajectoire optimale pour l'observateur}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width = 10 cm]{TP3/crameropti.png}
  \label{crameropti}
  \caption{Borne de Cramer Rao pour la trajectoire optimale}
\end{figure}

\subsection{Conclusion}
En conclusion, ce TP nous aura permis de voir comment la Borne de Cramer Rao permet de réaliser le choix d'une stratégie pour 
optimiser la précision sur l'estimation de état d'un système, grâce à la seule connaissance d'un modèle à priori.

\newpage

\section{TP4: titre du TP}
\subsection{Introduction}

\subsection{Titre intermédiaire}

\subsection{Conclusion}


\end{document}
